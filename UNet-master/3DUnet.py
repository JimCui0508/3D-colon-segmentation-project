# -*- coding: utf-8 -*-
"""3DUnet_BraTS2019_colab.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C0-djxJK1kp_qmLYPSC_CP0U69u46BxS
"""

#Code sourced from following links
#https://github.com/sarthak25/Brain-tumor-segmentation
#https://github.com/karolzak/keras-unet/tree/master/keras_unet
#https://github.com/MohamedAliHabib/Brain-Tumor-Detection
#https://github.com/charan223/Brain-Tumor-Segmentation-using-Topological-Loss
#https://github.com/sdsubhajitdas/Brain-Tumor-Segmentation
#https://github.com/Mehrdad-Noori/Brain-Tumor-Segmentation
#https://github.com/yunyuntsai/BraTS-brain-tumer-segmentation/blob/master/Seg_net.ipynb

import tensorflow as tf
from keras.models import Model
from keras.layers import Dense, Dropout, Activation, Flatten, concatenate, Conv2D, MaxPooling2D, Conv2DTranspose, Conv3D, MaxPooling3D, Conv3DTranspose
from keras.layers import Input, merge, UpSampling2D,BatchNormalization
from keras.callbacks import ModelCheckpoint, EarlyStopping
from keras.optimizers import Adam
from keras.preprocessing.image import ImageDataGenerator
from keras import backend as K
from keras.utils import multi_gpu_model
#import tensorflow as tf
import os
import SimpleITK as sitk
import matplotlib.pyplot as plt
import skimage.io as io
from glob import glob

import cupy as np
import numpy as npc
import random as r
import cv2
import keras.backend.tensorflow_backend as KTF



'''
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        # Currently, memory growth needs to be the same across GPUs
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
 #           tf.config.experimental.set_virtual_device_configuration(
  #      gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2560)])
        logical_gpus = tf.config.experimental.list_logical_devices('GPU')
        print(len(gpus), "Physical GPUs,", len(logical_gpus), "Logical GPUs")
    except RuntimeError as e:
        # Memory growth must be set before GPUs have been initialized
        print(e)
'''
os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"] = "3,7"
config = tf.ConfigProto()
#config.gpu_options.per_process_gpu_memory_fraction = 0.9 # 每个GPU上限控制在90%以内
session = tf.Session(config=config)
# 设置session
KTF.set_session(session)
path = "/home/public/CTC_ReconResults/zip_files/"


origin_files = glob(path+"/1.3.6.1.4.1.9328.50.4.*/**/ResampledImg.nii.gz", recursive=True)
seg_files = glob(path+"/1.3.6.1.4.1.9328.50.4.*/**/ResampledMask.nii.gz", recursive=True)


print(len(origin_files),len(seg_files))



"""# Convert to Array"""

first_img = io.imread(origin_files[1], plugin="simpleitk")

print(f"shape: {first_img.shape}")
print(f"dtype: {first_img.dtype}")

"""## Visualize Flair Image"""





"""## Visualize Segmented Image"""


seg_img = io.imread(seg_files[1], plugin="simpleitk")

print(f"shape: {seg_img.shape}")
print(f"dtype: {seg_img.dtype}")



"""# Convert to Array """


def to_array(path, end, label=False):

    # get locations
    files = glob(path+end, recursive=True)
    img_list = []
    
    r.seed(42)
    r.shuffle(files)
    
    for file in files:
        img = io.imread(file, plugin="simpleitk")
      #  img = np.array(img)
        # standardization
      #  img = (img-img.mean())/img.std()
      #  img.astype("float32")
       # print("img:",img.shape)
        img_list.append(img)
    
    img_list = np.array(img_list)  
    img_list = img_list.reshape((-1,64,128,128))    
    return img_list

"""### np.expand_dims()"""





"""# Applying the Function"""

train = to_array(path=path, end="/1.3.6.1.4.1.9328.50.4.*/**/ResampledImg.nii.gz")
print(f"dtype: {train.dtype}")

seg = to_array(path=path, end="/1.3.6.1.4.1.9328.50.4.*/**/ResampledMask.nii.gz", label=True)
print(f"dtype: {seg.dtype}")




#t1ce = to_array(path=path, end='**/*t1ce.nii.gz')

#plt.imshow(t1ce[550].reshape(128,128))
#plt.title("t1ce: 550");
'''
"""# Segmented Images

1 - Non-enhancing Tumor

2 - Edema 

4 - Enhancing Tumor
"""

seg_img.shape

plt.figure(figsize=(7,7))
plt.imshow(seg_img[100,:,:]);

seg_all = seg_img.copy()

seg_all[seg_all != 0] = 1 # show all tumor area

plt.figure(figsize=(7,7))
plt.imshow(seg_all[100,:,:])
plt.title("All area of Tumor", fontsize=15);

seg_non = seg_img.copy()

seg_non[seg_non != 1] = 0 # show non-enhancing tumor

plt.figure(figsize=(7,7))
plt.imshow(seg_non[100,:,:])
plt.title("1 - Non-enhancing Tumor", fontsize=15);

seg_edema = seg_img.copy()

seg_edema[seg_edema == 1] = 0 # cover Non-enhancing Tumor
seg_edema[seg_edema == 4] = 0 # cover Enhancing Tumor
seg_edema[seg_edema != 0] = 1 # show Edema

plt.figure(figsize=(7,7))
plt.imshow(seg_edema[100,:,:])
plt.title("2 - Edema", fontsize=15);

seg_enhancing = seg_img.copy()

seg_enhancing[seg_enhancing != 4] = 0 # show Enhancing Tumor

plt.figure(figsize=(7,7))
plt.imshow(seg_enhancing[100,:,:])
plt.title("Enhancing Tumor", fontsize=15);

"""# To Array for Segmented Images"""

def image_to_array(path, end, label):
    
    # get locations
    files = glob(path+end, recursive=True)
    
    single_img = []
    img_list = []
    
    r.seed(42)
    r.shuffle(files)
    
    for file in files:
        img = io.imread(file, plugin="simpleitk")
        
        # all tumor
        if label == 1:
            img[img != 0] = 1
        
        # Non-enhancing Tumor
        if label == 2:
            img[img != 1] = 0
        
        # Without Edema
        if label == 3:
            img[img == 2] = 0
            img[img != 0] = 1
        
        # Enhancing Tumor
        if label == 4:
            img[img != 4] = 0
            img[img == 4] = 1
            

        img.astype("float32")
        
        for slice in range(60, 124):
            img_s = img[slice,:,:]
            
            # resize
            img_s = cv2.resize(img_s, (128,128))
            single_img.append(img_s)
      #      img_s1 = np.expand_dims(img_s1, axis=0)
        img_list.append(single_img)
        single_img = []
          #  img_list.concatenate(img_s, axis=0)
            
    return np.array(img_list,np.float32)

seg_all_tumor = image_to_array(path=path, end="**/*seg.nii.gz", label=1)

#seg_non_enhancing = image_to_array(path=path, end="**/*seg.nii.gz", label=2)

#seg_out_edema = image_to_array(path=path, end="**/*seg.nii.gz", label=3) 

#seg_enhancing = image_to_array(path=path, end="**/*seg.nii.gz", label=4) 

#seg_original = image_to_array(path=path, end="**/*seg.nii.gz", label=0)

seg_all_tumor.shape

#seg_non_enhancing.shape

idx = 100

plt.figure(figsize=(20,15))

plt.subplot(3,4,1)
plt.imshow(seg_all_tumor[idx].reshape(128,128))
plt.title("seg_all_tumor")

#plt.subplot(3,4,2)
#plt.imshow(seg_non_enhancing[idx].reshape(128,128))
'''
#plt.title("seg_non_enhancing")

#plt.subplot(3,4,3)
#plt.imshow(seg_out_edema[idx].reshape(128,128))
#plt.title("seg_out_edema")

#plt.subplot(3,4,4)
#plt.imshow(seg_enhancing[idx].reshape(128,128))
#plt.title("seg_enhancing")

#plt.subplot(3,4,5)
#plt.imshow(seg_original[idx].reshape(128,128))
#plt.title("seg_original")

#plt.subplot(3,4,6)
#plt.imshow(train[idx].reshape(128,128))
#plt.title("train");

"""# U-NET MODEL"""

#flair = to_array(path=path, end="**/*flair.nii.gz")
#t2 = to_array(path=path, end="**/*t2.nii.gz")
#seg = image_to_array(path=path, end="**/*seg.nii.gz", label=1)

#flair.shape#, t2.shape #seg.shape

"""## Concatenate"""

#X_train = np.concatenate((flair, t2), axis=1)

#X_train = flair
train = np.expand_dims(train, axis=4)

#print(X_train.shape)
train = train.get()
seg = seg.get()


"""# U-NET MODEL"""

from tensorflow.keras.metrics import binary_crossentropy
from tensorflow.keras.metrics import AUC
from tensorflow.keras.metrics import Precision
from tensorflow.keras.metrics import Recall
from tensorflow.keras.metrics import MeanIoU

def iou(y_true, y_pred):
    smooth = 0.005 
    y_true_f = K.flatten(y_true)
    y_pred_f = K.flatten(y_pred)
    intersection = K.sum(y_true_f * y_pred_f)
    union = K.sum(y_true_f) + K.sum(y_pred_f) - K.sum(y_true_f * y_pred_f)
    return (intersection + smooth) / (union + smooth)

def tversky(y_true, y_pred):
    smooth = 0.005
    #Define alpha and beta
    alpha = 0.3
    beta = 0.7
    tp = K.sum(y_true * y_pred)
    fn = K.sum(y_true * (1-y_pred))
    fp = K.sum((1-y_true) * y_pred)
    return (tp + smooth)/(tp + alpha*fn + beta*fp + smooth)

from tensorflow.keras import layers

def dice_coef(y_true, y_pred):
  #  smooth = 0.005 
    smooth = 1e-5
    y_true_f = K.flatten(y_true)
    y_pred_f = K.flatten(y_pred)
    intersection = K.sum(y_true_f * y_pred_f)
    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)


def dice_coef_loss(y_true, y_pred):
    return 1-dice_coef(y_true, y_pred)
    

K.set_image_data_format('channels_last')


def unet():
    
    inputs = Input((64, 128 , 128, 1))
    
    conv1 = Conv3D(64, kernel_size=3, activation='relu', padding='same') (inputs)
    batch1 = BatchNormalization()(conv1)
    conv1 = Conv3D(64, kernel_size=3, activation='relu', padding='same') (batch1)
    batch1 = BatchNormalization()(conv1)
    pool1 = MaxPooling3D(pool_size=2)(batch1)
    
    conv2 = Conv3D(128, kernel_size=3, activation='relu', padding='same') (pool1)
    batch2 = BatchNormalization()(conv2)
    conv2 = Conv3D(128, kernel_size=3, activation='relu', padding='same') (batch2)
    batch2 = BatchNormalization()(conv2)
    pool2 = MaxPooling3D(pool_size=2)(batch2)
    
    conv3 = Conv3D(256, kernel_size=3, activation='relu', padding='same') (pool2)
    batch3 = BatchNormalization()(conv3)
    conv3 = Conv3D(256, kernel_size=3, activation='relu', padding='same') (batch3)
    batch3 = BatchNormalization()(conv3)
    pool3 = MaxPooling3D(pool_size=2)(batch3)
    
    conv4 = Conv3D(512, kernel_size=3, activation='relu', padding='same') (pool3)
    batch4 = BatchNormalization()(conv4)
    conv4 = Conv3D(512, kernel_size=3, activation='relu', padding='same') (batch4)
    batch4 = BatchNormalization()(conv4)
    pool4 = MaxPooling3D(pool_size=2)(batch4)
    
    conv5 = Conv3D(1024, kernel_size=3, activation='relu', padding='same') (pool4)
    batch5 = BatchNormalization()(conv5)
    conv5 = Conv3D(1024, kernel_size=3, activation='relu', padding='same') (batch5)
    batch5 = BatchNormalization()(conv5)
    
    up6 = Conv3DTranspose(512, (2, 2, 2), strides=(2, 2, 2), padding='same') (batch5)
    up6 = concatenate([up6, conv4], axis=4)
    conv6 = Conv3D(512, (3, 3, 3), activation='relu', padding='same') (up6)
    batch6 = BatchNormalization()(conv6)
    conv6 = Conv3D(512, (3, 3, 3), activation='relu', padding='same') (batch6)
    batch6 = BatchNormalization()(conv6)
    
    up7 = Conv3DTranspose(256, (2, 2, 2), strides=(2, 2, 2), padding='same') (batch6)
    up7 = concatenate([up7, conv3], axis=4)
    conv7 = Conv3D(256, kernel_size=3, activation='relu', padding='same') (up7)
    batch7 = BatchNormalization()(conv7)
    conv7 = Conv3D(256, kernel_size=3, activation='relu', padding='same') (batch7)
    batch7 = BatchNormalization()(conv7)
    
    up8 = Conv3DTranspose(128, (2, 2, 2), strides=(2, 2, 2), padding='same') (batch7)
    up8 = concatenate([up8, conv2], axis=4)
    conv8 = Conv3D(128, kernel_size=3, activation='relu', padding='same') (up8)
    batch8 = BatchNormalization()(conv8)
    conv8 = Conv3D(128, kernel_size=3, activation='relu', padding='same') (batch8)
    batch8 = BatchNormalization()(conv8)
    
    up9 = Conv3DTranspose(64, (2, 2, 2), strides=(2, 2, 2), padding='same') (batch8)
    up9 = concatenate([up9, conv1], axis=4)
    conv9 = Conv3D(64, kernel_size=3, activation='relu', padding='same') (up9)
    batch9 = BatchNormalization()(conv9)
    conv9 = Conv3D(64, kernel_size=3, activation='relu', padding='same') (batch9)
    batch9 = BatchNormalization()(conv9)

    conv10 = Conv3D(1, kernel_size=1, activation='sigmoid')(batch9)

    model = Model(inputs=[inputs], outputs=[conv10])

    

    return model


model = unet()
#model = multi_gpu_model(model, gpus=4)
model.compile(optimizer=Adam(learning_rate=1e-4), loss=dice_coef_loss, metrics=[dice_coef])
model.summary()

# Define callbacks.
checkpoint_cb = ModelCheckpoint(
    "3DUnet.h5", save_best_only=True
)
#early_stopping_cb = EarlyStopping(monitor="val_dice_coef", patience=50)



model.fit(train, seg, validation_split=0.25, batch_size=1, epochs=30, shuffle=True, callbacks=[checkpoint_cb])

model.save_weights("subset_model.h5")

plt.plot(model.history.history['loss'])

plt.plot(model.history.history['val_loss'])
plt.legend(['train', 'validation'], loc='upper right')
plt.ylim(0, 1)
plt.xlabel('epoch')
plt.ylabel('loss')
plt.title('model loss')
plt.show()





plt.plot(model.history.history['dice_coef'])
plt.plot(model.history.history['val_dice_coef'])
plt.title('dice score')
plt.ylim(0, 1)
plt.xlabel('epoch')
plt.ylabel('dice coefficient')
plt.legend(['train', 'validation'], loc='lower right')
plt.show()
'''
plt.plot(model.history.history['iou'])
plt.plot(model.history.history['val_iou'])
plt.ylim(0, 1)
plt.title('IoU')
plt.xlabel('epoch')
plt.ylabel('IoU score')
plt.legend(['train', 'validation'], loc='lower right')
plt.show()

plt.plot(model.history.history['recall'])
plt.plot(model.history.history['val_recall'])
plt.ylim(0, 1)
plt.title('Recall')
plt.xlabel('epoch')
plt.ylabel('Recall')
plt.legend(['train', 'validation'], loc='lower right')
plt.show()

plt.plot(model.history.history['precision'])
plt.plot(model.history.history['val_precision'])
plt.ylim(0, 1)
plt.title('Precision')
plt.xlabel('epoch')
plt.ylabel('Precision')
plt.legend(['train', 'validation'], loc='lower right')
plt.show()
'''


"""# PREDICTION"""
pred_file = "/home/public/CTC_ReconResults/zip_files/ResampledImg.nii.gz"

pred_img = io.imread(pred_file, plugin="simpleitk")
pred_img = np.array(pred_img)

expand_img = np.expand_dims(pred_img, axis=0)
expand_img = np.expand_dims(expand_img, axis=-1)

expand_img = expand_img.get()
pred = model.predict(expand_img)

## save 
out = sitk.GetImageFromArray(pred[0,:,:,:,0])

sitk.WriteImage(out,'/home/public/CTC_ReconResults/zip_files/simpleitk_save.nii.gz')






